{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtRHH5Q8RI6T",
        "outputId": "59b5f0ac-155a-42f1-e594-6f63773862f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers tokenizers sacrebleu -q\n",
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zNub1zWRLXk",
        "outputId": "13199a37-a82a-44b3-b5b2-493e1c2a8ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FvSpY3S5xEDy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from tokenizers import Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sacrebleu import corpus_bleu\n",
        "from google.colab import drive\n",
        "import os\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WShGXrKlxQEM"
      },
      "source": [
        "# Định nghĩa các lớp và hàm cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xVDghSLnxOq-"
      },
      "outputs": [],
      "source": [
        "class BaseSeq2Seq(nn.Module):\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate(self, src, max_len=100, beam_size=1):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GPIkDkchxX21"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if seq_len > self.pe.size(1):\n",
        "            raise ValueError(f\"Chiều dài chuỗi đầu vào ({seq_len}) vượt quá max_len của PositionalEncoding ({self.pe.size(1)}). Vui lòng tăng max_len trong PositionalEncoding hoặc giới hạn độ dài câu.\")\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sUk94tZgx4gQ"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(BaseSeq2Seq):\n",
        "    def __init__(self, input_dim, output_dim, embed_dim, num_heads, hidden_dim, num_layers, src_pad_idx, tgt_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.src_tok_emb = nn.Embedding(input_dim, embed_dim, padding_idx=src_pad_idx)\n",
        "        self.tgt_tok_emb = nn.Embedding(output_dim, embed_dim, padding_idx=tgt_pad_idx)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=0.3, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=0.3, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        tgt_len = tgt.size(1)\n",
        "        mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(self.device)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=1.0):\n",
        "        src_padding_mask = (src == self.src_pad_idx)\n",
        "        tgt_padding_mask = (tgt == self.tgt_pad_idx)\n",
        "\n",
        "        tgt_mask = self.make_tgt_mask(tgt).bool()\n",
        "\n",
        "        src_embed = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_embed = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "\n",
        "        memory = self.encoder(src_embed, src_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        output = self.decoder(\n",
        "            tgt_embed,\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask\n",
        "        )\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "    def generate(self, src, max_len=100, beam_size=1):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            with autocast():\n",
        "                src_padding_mask = (src == self.src_pad_idx).to(self.device)\n",
        "\n",
        "                src_embed = self.positional_encoding(self.src_tok_emb(src))\n",
        "                memory = self.encoder(src_embed, src_key_padding_mask=src_padding_mask)\n",
        "\n",
        "                batch_size = src.size(0)\n",
        "                sos_id = target_tokenizer.token_to_id(\"<sos>\")\n",
        "                eos_id = target_tokenizer.token_to_id(\"<eos>\")\n",
        "                pad_id = self.tgt_pad_idx\n",
        "\n",
        "                output_sequences = torch.full((batch_size, 1), fill_value=sos_id, dtype=torch.long, device=self.device)\n",
        "\n",
        "                finished_sequences = torch.zeros(batch_size, dtype=torch.bool, device=self.device)\n",
        "\n",
        "                for _ in range(max_len):\n",
        "                    if finished_sequences.all():\n",
        "                        break\n",
        "\n",
        "                    tgt_input = output_sequences\n",
        "\n",
        "                    tgt_embed = self.positional_encoding(self.tgt_tok_emb(tgt_input))\n",
        "                    tgt_mask = self.make_tgt_mask(tgt_input).bool()\n",
        "\n",
        "                    decoder_output = self.decoder(\n",
        "                        tgt_embed,\n",
        "                        memory,\n",
        "                        tgt_mask=tgt_mask,\n",
        "                        memory_key_padding_mask=src_padding_mask\n",
        "                    )\n",
        "\n",
        "                    output_logits = self.fc_out(decoder_output[:, -1, :])\n",
        "                    log_probs = F.log_softmax(output_logits, dim=-1)\n",
        "\n",
        "                    next_token_log_probs, next_token_ids = log_probs.max(dim=-1)\n",
        "\n",
        "                    next_token_ids = torch.where(finished_sequences, output_sequences[:, -1], next_token_ids)\n",
        "\n",
        "                    finished_sequences = finished_sequences | (next_token_ids == eos_id)\n",
        "\n",
        "                    output_sequences = torch.cat([output_sequences, next_token_ids.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "                final_results = []\n",
        "                for seq in output_sequences.tolist():\n",
        "                    clean_seq = []\n",
        "                    start_idx = 1 if seq[0] == sos_id else 0\n",
        "                    for token_id in seq[start_idx:]:\n",
        "                        if token_id == eos_id:\n",
        "                            break\n",
        "                        clean_seq.append(token_id)\n",
        "                    final_results.append(clean_seq)\n",
        "\n",
        "                if not final_results:\n",
        "                    return torch.full((batch_size, 1), fill_value=eos_id, dtype=torch.long, device=self.device)\n",
        "\n",
        "                max_len_out = max(len(seq) for seq in final_results)\n",
        "                output_tensor = torch.full(\n",
        "                    (batch_size, max_len_out),\n",
        "                    fill_value=pad_id,\n",
        "                    dtype=torch.long,\n",
        "                    device=self.device\n",
        "                )\n",
        "                for i, seq in enumerate(final_results):\n",
        "                    output_tensor[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=self.device)\n",
        "\n",
        "                return output_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3QRySw0yhZt"
      },
      "outputs": [],
      "source": [
        "class NoamScheduler:\n",
        "    def __init__(self, optimizer, model_size, warmup_steps):\n",
        "        self.optimizer = optimizer\n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.current_step = 0\n",
        "        self._rate = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.current_step += 1\n",
        "        lr = self.model_size**(-0.5) * min(self.current_step**(-0.5), self.current_step * self.warmup_steps**(-1.5))\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = lr\n",
        "        self._rate = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        return self.optimizer.param_groups[0]['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev9nyVPazDJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "global_wandb_step = 0\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device, scaler, clip_value=1.0, accumulation_steps=1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    loop = tqdm(dataloader, total=len(dataloader), desc=\"Huấn luyện\", leave=True, colour=\"green\")\n",
        "\n",
        "    global global_wandb_step\n",
        "\n",
        "    for i, batch in enumerate(loop):\n",
        "        src = batch['src'].to(device)\n",
        "        tgt = batch['tgt'].to(device)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        # Mixed precision training\n",
        "        with autocast():\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "            loss = criterion(output, tgt_output)\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer.optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            scaler.step(optimizer.optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_wandb_step += 1\n",
        "            wandb.log({\"Train Batch Loss\": loss.item() * accumulation_steps,\n",
        "                       \"Learning Rate\": optimizer.optimizer.param_groups[0]['lr']},\n",
        "                       step=global_wandb_step)\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "        loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CYN1zOOazIgD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        with autocast():\n",
        "            loop = tqdm(dataloader, total=len(dataloader), desc=\"Đánh giá\", leave=True, colour=\"blue\")\n",
        "            for batch in loop:\n",
        "                src = batch['src'].to(device)\n",
        "                tgt = batch['tgt'].to(device)\n",
        "                if tgt.dim() == 1:\n",
        "                    tgt = tgt.unsqueeze(0)\n",
        "                tgt_input = tgt[:, :-1]\n",
        "                tgt_output = tgt[:, 1:]\n",
        "\n",
        "                output = model(src, tgt_input)\n",
        "                output = output.reshape(-1, output.shape[-1])\n",
        "                tgt_output = tgt_output.reshape(-1)\n",
        "                loss = criterion(output, tgt_output)\n",
        "                total_loss += loss.item()\n",
        "                loop.set_postfix(loss=loss.item())\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fTSbvD3uzKPo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_model(name, config):\n",
        "    if name == 'transformer':\n",
        "        model_config = {\n",
        "            \"input_dim\": config[\"input_dim\"],\n",
        "            \"output_dim\": config[\"output_dim\"],\n",
        "            \"embed_dim\": config[\"embed_dim\"],\n",
        "            \"num_heads\": config[\"num_heads\"],\n",
        "            \"hidden_dim\": config[\"hidden_dim\"],\n",
        "            \"num_layers\": config[\"num_layers\"],\n",
        "            \"src_pad_idx\": config[\"src_pad_idx\"],\n",
        "            \"tgt_pad_idx\": config[\"tgt_pad_idx\"],\n",
        "            \"device\": config[\"device\"]\n",
        "        }\n",
        "        return Seq2SeqTransformer(**model_config)\n",
        "    else:\n",
        "        raise ValueError(f\"Tên mô hình không xác định: {name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYOc4FedzNHR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_bleu(model, valid_dataset, device, source_tokenizer, target_tokenizer, config, num_examples_to_print=5, num_subsets=10):\n",
        "    model.eval()\n",
        "    overall_hypotheses = []\n",
        "    overall_references = []\n",
        "    overall_original_sources = []\n",
        "    bleu_scores_per_subset = []\n",
        "\n",
        "    tgt_pad_idx = target_tokenizer.token_to_id(\"<pad>\")\n",
        "    tgt_sos_idx = target_tokenizer.token_to_id(\"<sos>\")\n",
        "    tgt_eos_idx = target_tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "    src_pad_idx = source_tokenizer.token_to_id(\"<pad>\")\n",
        "    src_sos_idx = source_tokenizer.token_to_id(\"<sos>\")\n",
        "    src_eos_idx = source_tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "    total_examples = len(valid_dataset)\n",
        "    subset_size = total_examples // num_subsets\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with autocast():\n",
        "            for i in range(num_subsets):\n",
        "                start_idx = i * subset_size\n",
        "                end_idx = (i + 1) * subset_size\n",
        "                if i == num_subsets - 1:\n",
        "                    end_idx = total_examples\n",
        "\n",
        "                current_subset_indices = list(range(start_idx, end_idx))\n",
        "                current_subset = Subset(valid_dataset, current_subset_indices)\n",
        "                current_dataloader = DataLoader(current_subset, batch_size=64, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "                subset_hypotheses = []\n",
        "                subset_references = []\n",
        "                subset_original_sources = []\n",
        "\n",
        "                loop = tqdm(current_dataloader, desc=f\"Tính BLEU Tập con {i+1}/{num_subsets}\", leave=False, colour=\"magenta\")\n",
        "                for batch in loop:\n",
        "                    src = batch['src'].to(device)\n",
        "                    tgt = batch['tgt'].to(device)\n",
        "\n",
        "                    output_ids = model.generate(src, max_len=config['max_seq_len_generation'], beam_size=1)\n",
        "\n",
        "                    for src_ids_sample, ref_ids_sample, hyp_ids_sample in zip(src.tolist(), tgt.tolist(), output_ids.tolist()):\n",
        "                        src_text = source_tokenizer.decode(src_ids_sample, skip_special_tokens=True)\n",
        "                        subset_original_sources.append(src_text)\n",
        "\n",
        "                        ref_text = target_tokenizer.decode(ref_ids_sample, skip_special_tokens=True)\n",
        "                        subset_references.append(ref_text)\n",
        "\n",
        "                        hyp_text = target_tokenizer.decode(hyp_ids_sample, skip_special_tokens=True)\n",
        "                        subset_hypotheses.append(hyp_text)\n",
        "\n",
        "                if subset_hypotheses and subset_references:\n",
        "                    subset_bleu = corpus_bleu(subset_hypotheses, [subset_references])\n",
        "                    bleu_scores_per_subset.append(subset_bleu.score)\n",
        "                    print(f\"  - Điểm BLEU tập thứ {i+1}: {subset_bleu.score:.2f}\")\n",
        "                    wandb.log({f\"BLEU_Subset_{i+1}\": subset_bleu.score}, step=global_wandb_step)\n",
        "\n",
        "                    overall_hypotheses.extend(subset_hypotheses)\n",
        "                    overall_references.extend(subset_references)\n",
        "                    overall_original_sources.extend(subset_original_sources)\n",
        "                else:\n",
        "                    print(f\"  - Tập con thứ {i+1} không có đủ dữ liệu để tính BLEU.\")\n",
        "\n",
        "    if overall_hypotheses and overall_references:\n",
        "        overall_bleu = corpus_bleu(overall_hypotheses, [overall_references])\n",
        "        print(f\"\\n- Điểm BLEU Trung bình (tất cả tập con): {sum(bleu_scores_per_subset) / len(bleu_scores_per_subset):.2f}\")\n",
        "        print(f\"- Điểm BLEU Tổng thể: {overall_bleu.score:.2f}\")\n",
        "    else:\n",
        "        print(\"\\nKhông có dữ liệu tổng thể để tính BLEU.\")\n",
        "        return 0.0\n",
        "\n",
        "    print(\"\\n- Bản dịch mẫu:\")\n",
        "    for i in range(min(num_examples_to_print, len(overall_hypotheses))):\n",
        "        print(f\"Gốc (SRC): {overall_original_sources[i]}\")\n",
        "        print(f\"Tham chiếu (REF): {overall_references[i]}\")\n",
        "        print(f\"Dịch máy (HYP): {overall_hypotheses[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return overall_bleu.score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wnup3AjEzq3l"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src = src_data\n",
        "        self.tgt = tgt_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'src': torch.tensor(self.src[idx], dtype=torch.long),\n",
        "            'tgt': torch.tensor(self.tgt[idx], dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6pAdsm-ztCH"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    src_pad_id = source_tokenizer.token_to_id(\"<pad>\")\n",
        "    tgt_pad_id = target_tokenizer.token_to_id(\"<pad>\")\n",
        "\n",
        "    src_batch = [item['src'] for item in batch]\n",
        "    tgt_batch = [item['tgt'] for item in batch]\n",
        "\n",
        "    src_pad = pad_sequence(src_batch, batch_first=True, padding_value=src_pad_id)\n",
        "    tgt_pad = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_pad_id)\n",
        "    return {\n",
        "        'src': src_pad,\n",
        "        'tgt': tgt_pad\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaMEt_SmzvcD"
      },
      "source": [
        "# CẤU HÌNH VÀ TẢI DỮ LIỆU/TOKENIZER TRÊN COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS7D5JX2z5bu",
        "outputId": "b04566de-3844-4832-f39d-888d8f2719bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "--- Thông tin về dữ liệu đã tải ---\n",
            "Số lượng cặp câu huấn luyện: 100001\n",
            "Số lượng cặp câu validation: 10001\n",
            "Độ dài câu nguồn trung bình: 22.68\n",
            "Độ dài câu nguồn tối đa: 202\n",
            "Độ dài câu đích trung bình: 25.42\n",
            "Độ dài câu đích tối đa: 259\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "GOOGLE_DRIVE_PATH = '/content/drive/MyDrive/NMT_Project_Files'\n",
        "\n",
        "try:\n",
        "    source_tokenizer = Tokenizer.from_file(os.path.join(GOOGLE_DRIVE_PATH, '300kenglish_bpe_bytelevel.json'))\n",
        "    target_tokenizer = Tokenizer.from_file(os.path.join(GOOGLE_DRIVE_PATH, '300kvietnamese_bpe_bytelevel.json'))\n",
        "\n",
        "    from tokenizers.processors import BertProcessing\n",
        "    source_tokenizer.post_processor = BertProcessing(\n",
        "        sep=(\"<eos>\", source_tokenizer.token_to_id(\"<eos>\")),\n",
        "        cls=(\"<sos>\", source_tokenizer.token_to_id(\"<sos>\")),\n",
        "    )\n",
        "    target_tokenizer.post_processor = BertProcessing(\n",
        "        sep=(\"<eos>\", target_tokenizer.token_to_id(\"<eos>\")),\n",
        "        cls=(\"<sos>\", target_tokenizer.token_to_id(\"<sos>\")),\n",
        "    )\n",
        "\n",
        "    if source_tokenizer.token_to_id(\"<unk>\") is None:\n",
        "        source_tokenizer.add_special_tokens([\"<unk>\"])\n",
        "    if target_tokenizer.token_to_id(\"<unk>\") is None:\n",
        "        target_tokenizer.add_special_tokens([\"<unk>\"])\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi tải Tokenizer: {e}\")\n",
        "    print(\"Vui lòng kiểm tra đường dẫn và tên file tokenizer cũng như các token đặc biệt.\")\n",
        "    exit()\n",
        "src_ids = []\n",
        "tgt_ids = []\n",
        "src_ids_validate = []\n",
        "tgt_ids_validate = []\n",
        "\n",
        "temp_src_ids = []\n",
        "temp_tgt_ids = []\n",
        "try:\n",
        "    with open(os.path.join(GOOGLE_DRIVE_PATH, \"100kenglish.txt\"), \"r\", encoding=\"utf-8\") as f_src, \\\n",
        "         open(os.path.join(GOOGLE_DRIVE_PATH, \"100kvietnam.txt\"), \"r\", encoding=\"utf-8\") as f_tgt:\n",
        "        for i, (src_line, tgt_line) in enumerate(zip(f_src, f_tgt)):\n",
        "            src_line = src_line.strip()\n",
        "            tgt_line = tgt_line.strip()\n",
        "            if src_line and tgt_line:\n",
        "                src_output = source_tokenizer.encode(src_line)\n",
        "                src_id = src_output.ids\n",
        "                temp_src_ids.append(src_id)\n",
        "\n",
        "                tgt_output = target_tokenizer.encode(tgt_line)\n",
        "                tgt_id = tgt_output.ids\n",
        "                temp_tgt_ids.append(tgt_id)\n",
        "            elif not src_line and not tgt_line:\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Cảnh báo: Dòng {i+1} trong tập huấn luyện có sự không khớp (một dòng rỗng, một không rỗng). Bỏ qua cặp này.\")\n",
        "    src_ids = temp_src_ids\n",
        "    tgt_ids = temp_tgt_ids\n",
        "    if len(src_ids) != len(tgt_ids):\n",
        "        print(f\"CẢNH BÁO: Số lượng câu nguồn ({len(src_ids)}) và câu đích ({len(tgt_ids)}) không khớp sau khi tiền xử lý.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Lỗi FileNotFoundError khi tải dữ liệu huấn luyện: {e}. Vui lòng kiểm tra đường dẫn.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi không xác định khi tải dữ liệu huấn luyện: {e}\")\n",
        "    exit()\n",
        "\n",
        "temp_src_ids_validate = []\n",
        "temp_tgt_ids_validate = []\n",
        "try:\n",
        "    with open(os.path.join(GOOGLE_DRIVE_PATH, \"english_validate.txt\"), \"r\", encoding=\"utf-8\") as f_src_val, \\\n",
        "         open(os.path.join(GOOGLE_DRIVE_PATH, \"vietnam_validate.txt\"), \"r\", encoding=\"utf-8\") as f_tgt_val:\n",
        "        for i, (src_line_val, tgt_line_val) in enumerate(zip(f_src_val, f_tgt_val)):\n",
        "            src_line_val = src_line_val.strip()\n",
        "            tgt_line_val = tgt_line_val.strip()\n",
        "            if src_line_val and tgt_line_val:\n",
        "                src_output_val = source_tokenizer.encode(src_line_val)\n",
        "                src_id_val = src_output_val.ids\n",
        "                temp_src_ids_validate.append(src_id_val)\n",
        "\n",
        "                tgt_output_val = target_tokenizer.encode(tgt_line_val)\n",
        "                tgt_id_val = tgt_output_val.ids\n",
        "                temp_tgt_ids_validate.append(tgt_id_val)\n",
        "            elif not src_line_val and not tgt_line_val:\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Cảnh báo: Dòng validation {i+1} có sự không khớp (một dòng rỗng, một không rỗng). Bỏ qua cặp này.\")\n",
        "    src_ids_validate = temp_src_ids_validate\n",
        "    tgt_ids_validate = temp_tgt_ids_validate\n",
        "    if len(src_ids_validate) != len(tgt_ids_validate):\n",
        "        print(f\"CẢNH BÁO: Số lượng câu nguồn validation ({len(src_ids_validate)}) và câu đích validation ({len(tgt_ids_validate)}) không khớp sau khi tiền xử lý.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Lỗi FileNotFoundError khi tải dữ liệu validation: {e}. Vui lòng kiểm tra đường dẫn.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi không xác định khi tải dữ liệu validation: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\n--- Thông tin về dữ liệu đã tải ---\")\n",
        "print(f\"Số lượng cặp câu huấn luyện: {len(src_ids)}\")\n",
        "print(f\"Số lượng cặp câu validation: {len(src_ids_validate)}\")\n",
        "\n",
        "avg_src_len = sum(len(s) for s in src_ids) / len(src_ids) if len(src_ids) > 0 else 0\n",
        "max_src_len = max(len(s) for s in src_ids) if len(src_ids) > 0 else 0\n",
        "avg_tgt_len = sum(len(t) for t in tgt_ids) / len(tgt_ids) if len(tgt_ids) > 0 else 0\n",
        "max_tgt_len = max(len(t) for t in tgt_ids) if len(tgt_ids) > 0 else 0\n",
        "\n",
        "print(f\"Độ dài câu nguồn trung bình: {avg_src_len:.2f}\")\n",
        "print(f\"Độ dài câu nguồn tối đa: {max_src_len}\")\n",
        "print(f\"Độ dài câu đích trung bình: {avg_tgt_len:.2f}\")\n",
        "print(f\"Độ dài câu đích tối đa: {max_tgt_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38YP6Do-0Nuy"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = TranslationDataset(src_ids, tgt_ids)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "validate = TranslationDataset(src_ids_validate, tgt_ids_validate)\n",
        "full_valid_dataloader = DataLoader(validate, batch_size=64, shuffle=False, collate_fn=collate_fn, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXt56Y781d7s"
      },
      "source": [
        "# Cấu hình mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "v1qMUKlI1nHr",
        "outputId": "aac65e3e-eff2-43ca-a579-3950f974a391"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtonthanhdat05092000\u001b[0m (\u001b[33mtonthanhdat05092000-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250630_131742-pcobubiu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT/runs/pcobubiu' target=\"_blank\">vital-wildflower-10</a></strong> to <a href='https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT' target=\"_blank\">https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT/runs/pcobubiu' target=\"_blank\">https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT/runs/pcobubiu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Thông số Mô hình Transformer ---\n",
            "Embedding Dimension (embed_dim): 256\n",
            "Number of Heads (num_heads): 8\n",
            "Feedforward Dimension (hidden_dim): 512\n",
            "Number of Layers (num_layers): 6\n",
            "Tổng số tham số có thể huấn luyện: 20,211,328\n",
            "\n",
            "Đã tải 'best_model.pt' để tiếp tục huấn luyện.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-16-727646305.py:62: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    \"input_dim\": source_tokenizer.get_vocab_size(),\n",
        "    \"output_dim\": target_tokenizer.get_vocab_size(),\n",
        "    \"embed_dim\": 256,\n",
        "    \"hidden_dim\": 512,\n",
        "    \"num_layers\": 6,\n",
        "    \"num_heads\": 8,\n",
        "    \"src_pad_idx\": source_tokenizer.token_to_id(\"<pad>\"),\n",
        "    \"tgt_pad_idx\": target_tokenizer.token_to_id(\"<pad>\"),\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"warmup_steps\": 4000,\n",
        "    \"beam_size\": 1,\n",
        "    \"accumulation_steps\": 4,\n",
        "    \"max_seq_len_generation\": 100\n",
        "}\n",
        "num_epochs = 30\n",
        "\n",
        "wandb.login() \n",
        "wandb.init(project=\"NMT_Transformer_PhoMT\", config=config)\n",
        "\n",
        "wandb.config.update({\n",
        "    \"source_vocab_size\": config[\"input_dim\"],\n",
        "    \"target_vocab_size\": config[\"output_dim\"],\n",
        "    \"model_embedding_dim\": config[\"embed_dim\"],\n",
        "    \"model_num_heads\": config[\"num_heads\"],\n",
        "    \"model_feedforward_dim\": config[\"hidden_dim\"],\n",
        "    \"model_num_layers\": config[\"num_layers\"],\n",
        "    \"training_warmup_steps\": config[\"warmup_steps\"],\n",
        "    \"inference_beam_size\": config[\"beam_size\"],\n",
        "    \"training_gradient_accumulation_steps\": config[\"accumulation_steps\"],\n",
        "    \"training_device\": config[\"device\"].type,\n",
        "    \"max_sequence_length_generation\": config[\"max_seq_len_generation\"]\n",
        "})\n",
        "\n",
        "\n",
        "model = get_model(\"transformer\", config).to(config[\"device\"])\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n--- Thông số Mô hình Transformer ---\")\n",
        "print(f\"Embedding Dimension (embed_dim): {config['embed_dim']}\")\n",
        "print(f\"Number of Heads (num_heads): {config['num_heads']}\")\n",
        "print(f\"Feedforward Dimension (hidden_dim): {config['hidden_dim']}\")\n",
        "print(f\"Number of Layers (num_layers): {config['num_layers']}\")\n",
        "print(f\"Tổng số tham số có thể huấn luyện: {total_params:,}\")\n",
        "\n",
        "\n",
        "try:\n",
        "    model_path = os.path.join(GOOGLE_DRIVE_PATH, \"best_model.pt\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=config[\"device\"]))\n",
        "    print(\"\\nĐã tải 'best_model.pt' để tiếp tục huấn luyện.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nKHÔNG TÌM THẤY FILE 'best_model.pt'. Mô hình sẽ bắt đầu huấn luyện từ đầu với các trọng số ngẫu nhiên.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nLỗi khi tải mô hình: {e}\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=config[\"tgt_pad_idx\"])\n",
        "scaler = GradScaler()\n",
        "scheduler = NoamScheduler(optimizer, model_size=config[\"embed_dim\"], warmup_steps=config[\"warmup_steps\"])\n",
        "\n",
        "patience = 6\n",
        "counter = 0\n",
        "best_val_loss = float('inf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2gmudYh3Q0N"
      },
      "source": [
        "# Huấn luyện"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHzLXBx03uLm",
        "outputId": "89bec645-43e5-4529-95e2-26f62508dbed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rHuấn luyện:   0%|\u001b[32m          \u001b[0m| 0/1563 [00:00<?, ?it/s]/tmp/ipython-input-8-1141824594.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:26<00:00, 10.68it/s, loss=1.99]\n",
            "/tmp/ipython-input-9-218587686.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:04<00:00, 34.82it/s, loss=2.82]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7556 | Val Loss: 3.0087\n",
            "Đã lưu mô hình tốt nhất mới tại epoch 1 với val_loss: 3.0087 vào '/content/drive/MyDrive/NMT_Project_Files/new_best_model.pt'\n",
            "\n",
            "Epoch: 2/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:24<00:00, 10.84it/s, loss=1.65]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:03<00:00, 41.97it/s, loss=2.82]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7423 | Val Loss: 3.0098\n",
            "Val Loss không cải thiện trong 1/6 epoch.\n",
            "\n",
            "Epoch: 3/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:23<00:00, 10.86it/s, loss=1.69]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:03<00:00, 42.24it/s, loss=2.81]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7338 | Val Loss: 3.0099\n",
            "Val Loss không cải thiện trong 2/6 epoch.\n",
            "\n",
            "Epoch: 4/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:23<00:00, 10.91it/s, loss=1.72]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:04<00:00, 33.68it/s, loss=2.81]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7239 | Val Loss: 3.0014\n",
            "Đã lưu mô hình tốt nhất mới tại epoch 4 với val_loss: 3.0014 vào '/content/drive/MyDrive/NMT_Project_Files/new_best_model.pt'\n",
            "\n",
            "Epoch: 5/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:25<00:00, 10.77it/s, loss=1.77]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:04<00:00, 37.19it/s, loss=2.81]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7174 | Val Loss: 3.0025\n",
            "Val Loss không cải thiện trong 1/6 epoch.\n",
            "\n",
            "Epoch: 6/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:23<00:00, 10.86it/s, loss=1.95]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:03<00:00, 42.04it/s, loss=2.82]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7088 | Val Loss: 3.0134\n",
            "Val Loss không cải thiện trong 2/6 epoch.\n",
            "\n",
            "Epoch: 7/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:24<00:00, 10.85it/s, loss=1.63]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:03<00:00, 40.15it/s, loss=2.82]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7010 | Val Loss: 3.0164\n",
            "Val Loss không cải thiện trong 3/6 epoch.\n",
            "\n",
            "Epoch: 8/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:23<00:00, 10.86it/s, loss=1.67]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:04<00:00, 34.68it/s, loss=2.81]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6934 | Val Loss: 3.0149\n",
            "Val Loss không cải thiện trong 4/6 epoch.\n",
            "\n",
            "Epoch: 9/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:23<00:00, 10.91it/s, loss=1.82]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:03<00:00, 39.68it/s, loss=2.79]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6881 | Val Loss: 3.0118\n",
            "Val Loss không cải thiện trong 5/6 epoch.\n",
            "\n",
            "Epoch: 10/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Huấn luyện: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [02:25<00:00, 10.77it/s, loss=1.67]\n",
            "Đánh giá: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:03<00:00, 39.96it/s, loss=2.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6810 | Val Loss: 3.0100\n",
            "Val Loss không cải thiện trong 6/6 epoch.\n",
            "Early stopping do val_loss không cải thiện.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
        "\n",
        "    train_loss = train_epoch(model, dataloader, scheduler, criterion, config[\"device\"], scaler,\n",
        "                             clip_value=1.0, accumulation_steps=config[\"accumulation_steps\"])\n",
        "\n",
        "    val_loss = validate_epoch(model, full_valid_dataloader, criterion, config[\"device\"])\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"Epoch\": epoch + 1,\n",
        "        \"Train Loss (Epoch)\": train_loss,\n",
        "        \"Validation Loss (Epoch)\": val_loss,\n",
        "        \"Learning Rate (Epoch End)\": scheduler.get_last_lr()\n",
        "    }, step=global_wandb_step)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        save_path = os.path.join(GOOGLE_DRIVE_PATH, 'new_best_model.pt')\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Đã lưu mô hình tốt nhất mới tại epoch {epoch+1} với val_loss: {best_val_loss:.4f} vào '{save_path}'\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"Val Loss không cải thiện trong {counter}/{patience} epoch.\")\n",
        "        if counter >= patience:\n",
        "            print('Early stopping do val_loss không cải thiện.')\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA_gdqd73xNA"
      },
      "source": [
        "# Đánh giá BLEU trên mô hình tốt nhất sau khi huấn luyện"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae1t0Dlxom0O",
        "outputId": "89a7f874-9f06-43e4-ad9f-85642fbf9db4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-11-1344781054.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã tải mô hình tốt nhất từ '/content/drive/MyDrive/NMT_Project_Files/new_best_model.pt' để tính BLEU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTính BLEU Tập con 1/10:   0%|\u001b[35m          \u001b[0m| 0/16 [00:00<?, ?it/s]/tmp/ipython-input-6-3207906614.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 1: 20.80\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 2: 24.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 3: 22.18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 4: 16.08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 5: 14.47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 6: 17.82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 7: 13.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 8: 15.91\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 9: 14.22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Điểm BLEU tập thứ 10: 14.67\n",
            "\n",
            "- Điểm BLEU Trung bình (tất cả tập con): 17.39\n",
            "- Điểm BLEU Tổng thể: 19.03\n",
            "\n",
            "- Bản dịch mẫu:\n",
            "Gốc (SRC):  brother albert barnett and his wife, sister susan barnett, from the west congregation in tuscaloosa, alabama\n",
            "Tham chiếu (REF):  anh albert barnett và chị susan barnett, thuộc hội thánh west ở tuscaloosa, alabama\n",
            "Dịch máy (HYP):  anh trai rượu và vợ của anh, chị gái rút lui, từ tây âu, alabama, alabama.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  severe storms ripped through parts of the southern and midwestern united states on january 11 and 12, 2020.\n",
            "Tham chiếu (REF):  ngày 11 và 12-1-2020, những cơn bão lớn đã quét qua và phá huỷ nhiều vùng ở miền nam và miền trung hoa kỳ.\n",
            "Dịch máy (HYP):  cảnh quay vô tình xuyên qua những phần của nam và mỹ ở 11  11  năm 2020 và năm 2020.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  two days of heavy rain, high winds, and numerous tornadoes caused major damage across multiple states.\n",
            "Tham chiếu (REF):  những trận mưa to và gió lớn trong suốt hai ngày cùng với nhiều cơn lốc xoáy đã gây thiệt hại nặng nề cho nhiều bang.\n",
            "Dịch máy (HYP):  hai ngày mưa nặng, gió, gió và nhiều cơn bão cao và bị hư hại lớn gây ra tổn thất lớn trên nhiều quốc gia.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  sadly, brother albert barnett and his wife, sister susan barnett, 85 and 75 years old respectively, were killed when a tornado struck their mobile home.\n",
            "Tham chiếu (REF):  đáng buồn là anh albert barnett 85 tuổi, và vợ anh là chị susan barnett 75 tuổi đã thiệt mạng do một cơn lốc xoáy quét qua nhà họ.\n",
            "Dịch máy (HYP):  buồn thay, anh trai tôi và vợ, chị gái ba lô, 85o and 75 tuổi, bị giết khi một trận động vật bị đánh bại trong nhà.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  the united states branch also reports that at least four of our brothers' homes sustained minor damage, along with two kingdom halls.\n",
            "Tham chiếu (REF):  chi nhánh hoa kỳ cũng cho biết có ít nhất bốn căn nhà của anh em chúng tôi và hai phòng nước trời bị hư hại nhẹ.\n",
            "Dịch máy (HYP):  các nhà nước mỹ cũng nói rằng ít nhất bốn trong số những người anh em nhà của chúng tôi bị thiệt hại, cùng với hai hội trường.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  additionally, the storms caused major damage to a brother's business property.\n",
            "Tham chiếu (REF):  ngoài ra, những cơn bão cũng gây hư hại lớn cho cơ sở kinh doanh của một anh em.\n",
            "Dịch máy (HYP):  thêm vào đó, ảnh hưởng gây ra thiệt hại cho em trai của em trai.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  local elders and the circuit overseer are offering practical and spiritual support to those affected by this disaster.\n",
            "Tham chiếu (REF):  các trưởng lão địa phương và giám thị xung quanh đang giúp đỡ và cung cấp về vật chất và tinh thần cho các anh chị bị ảnh hưởng trong thảm hoạ này.\n",
            "Dịch máy (HYP):  nhà địa phương và mạch cạnh tranh là sự nhạy cảm và tâm linh và sự ủng hộ cho những tai hoạ này bị ảnh hưởng bởi thảm hoạ này.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  we know that our heavenly father, jehovah, is providing comfort to our brothers and sisters who are grieving because of this tragedy.\n",
            "Tham chiếu (REF):  chúng ta tin chắc rằng cha trên trời, đức giê-hô-va, đang an ủi những anh chị em của chúng ta trong cảnh đau buồn.\n",
            "Dịch máy (HYP):  chúng ta biết rằng thiên chúa của chúng ta, trách nhiệm, là cung cấp thoải mái với anh chị em và chị em họ đang đau khổ vì bi kịch này.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  international government agencies and officials have responded to russia's supreme court decision that criminalizes the worship of jehovah's witnesses in russia.\n",
            "Tham chiếu (REF):  các cơ quan và viên chức chính phủ quốc tế đã lên tiếng trước phán quyết của toà tối cao nga về việc cấm sự thờ phượng của nhân chứng giê-hô-va ở nga.\n",
            "Dịch máy (HYP):  chính phủ quốc tế và các nhà nước có quyền trả lời quyết định của nga rằng sự tôn thờ của sự tôn thờ ơ của quỷ – ở nga.\n",
            "--------------------------------------------------\n",
            "Gốc (SRC):  these statements have criticized russia's unjust and harsh judicial action against a minority religious group known for peaceful religious activity.\n",
            "Tham chiếu (REF):  các lời nhận xét chỉ trích nước nga có hành động tư pháp khắc nghiệt và bất công nhắm vào một nhóm tôn giáo nhỏ được biết đến là hoạt động một cách ôn hoà.\n",
            "Dịch máy (HYP):  những người này đã bị xã hội sụp đổ và thiếu đạo thiên văn học chống lại một nhóm tôn giáo được biết đến với hoạt động bình đẳng tôn giáo.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    final_best_model_path = os.path.join(GOOGLE_DRIVE_PATH, 'new_best_model.pt')\n",
        "    if os.path.exists(final_best_model_path):\n",
        "        model.load_state_dict(torch.load(final_best_model_path, map_location=config[\"device\"]))\n",
        "        print(f\"Đã tải mô hình tốt nhất từ '{final_best_model_path}' để tính BLEU.\")\n",
        "\n",
        "        final_bleu_score = compute_bleu(model, validate, config[\"device\"],\n",
        "                                        source_tokenizer, target_tokenizer, config, num_examples_to_print=10)\n",
        "\n",
        "    else:\n",
        "        print(f\"Không tìm thấy file mô hình tốt nhất tại '{final_best_model_path}'. Không thể tính BLEU.\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi tải lại mô hình hoặc tính BLEU: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "s3BqWIRMz6ib",
        "outputId": "5ffad976-49c1-46cb-fb0e-68ad7d65b8ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BLEU_Subset_1</td><td>▁</td></tr><tr><td>BLEU_Subset_10</td><td>▁</td></tr><tr><td>BLEU_Subset_2</td><td>▁</td></tr><tr><td>BLEU_Subset_3</td><td>▁</td></tr><tr><td>BLEU_Subset_4</td><td>▁</td></tr><tr><td>BLEU_Subset_5</td><td>▁</td></tr><tr><td>BLEU_Subset_6</td><td>▁</td></tr><tr><td>BLEU_Subset_7</td><td>▁</td></tr><tr><td>BLEU_Subset_8</td><td>▁</td></tr><tr><td>BLEU_Subset_9</td><td>▁</td></tr><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Learning Rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning Rate (Epoch End)</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Batch Loss</td><td>▂▃▄▃▅▄▆▄▄▄▅▁▅▂▂▃▃▅▄▃█▄▆▆▄▅▄▄▂▅▄▂▃▃▂▅▄▃▃▃</td></tr><tr><td>Train Loss (Epoch)</td><td>█▇▆▅▄▄▃▂▂▁</td></tr><tr><td>Validation Loss (Epoch)</td><td>▄▅▅▁▂▇█▇▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BLEU_Subset_1</td><td>20.80145</td></tr><tr><td>BLEU_Subset_10</td><td>14.66862</td></tr><tr><td>BLEU_Subset_2</td><td>24.01341</td></tr><tr><td>BLEU_Subset_3</td><td>22.18401</td></tr><tr><td>BLEU_Subset_4</td><td>16.0761</td></tr><tr><td>BLEU_Subset_5</td><td>14.47068</td></tr><tr><td>BLEU_Subset_6</td><td>17.81717</td></tr><tr><td>BLEU_Subset_7</td><td>13.7567</td></tr><tr><td>BLEU_Subset_8</td><td>15.91164</td></tr><tr><td>BLEU_Subset_9</td><td>14.2182</td></tr><tr><td>Epoch</td><td>10</td></tr><tr><td>Learning Rate</td><td>0.0001</td></tr><tr><td>Learning Rate (Epoch End)</td><td>0.0001</td></tr><tr><td>Train Batch Loss</td><td>1.72807</td></tr><tr><td>Train Loss (Epoch)</td><td>1.681</td></tr><tr><td>Validation Loss (Epoch)</td><td>3.01003</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vital-wildflower-10</strong> at: <a href='https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT/runs/pcobubiu' target=\"_blank\">https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT/runs/pcobubiu</a><br> View project at: <a href='https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT' target=\"_blank\">https://wandb.ai/tonthanhdat05092000-/NMT_Transformer_PhoMT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250630_131742-pcobubiu/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "wandb.finish();"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
