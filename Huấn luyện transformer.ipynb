{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "transformer3.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dangduong2806/NMT-project/blob/main/Hu%E1%BA%A5n%20luy%E1%BB%87n%20transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nếu đã tải các file về local"
      ],
      "metadata": {
        "id": "i7EDVERQCcJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM-0_L1XCagR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "5bac781b-6827-4aee-983f-608041b7eda1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-70f35f86-9e3f-45ca-9def-e977996285e4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-70f35f86-9e3f-45ca-9def-e977996285e4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vietnam_validate.txt to vietnam_validate.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tokenizer cho mỗi language"
      ],
      "metadata": {
        "id": "e6SdjiNEGuZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer"
      ],
      "metadata": {
        "id": "N_bHZB87D04P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_tokenizer = Tokenizer.from_file('/content/300kenglish_bpe_bytelevel.json')\n",
        "target_tokenizer = Tokenizer.from_file('/content/300kvietnamese_bpe_bytelevel.json')"
      ],
      "metadata": {
        "id": "VghpMakYDrwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tạo list các id của tập training"
      ],
      "metadata": {
        "id": "P2jv-qwC9Q1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_ids = []\n",
        "with open(\"100kenglish.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      output = source_tokenizer.encode(line)\n",
        "      id = output.ids\n",
        "      sos_id = source_tokenizer.token_to_id(\"<sos>\")\n",
        "      eos_id = source_tokenizer.token_to_id(\"<eos>\")\n",
        "      id = [sos_id] + id + [eos_id]\n",
        "      src_ids.append(id)"
      ],
      "metadata": {
        "id": "CtETA0TJDZMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_ids = []\n",
        "with open(\"100kvietnam.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      output = target_tokenizer.encode(line)\n",
        "      id = output.ids\n",
        "      sos_id = target_tokenizer.token_to_id(\"<sos>\")\n",
        "      eos_id = target_tokenizer.token_to_id(\"<eos>\")\n",
        "      id = [sos_id] + id + [eos_id]\n",
        "      tgt_ids.append(id)"
      ],
      "metadata": {
        "id": "iwlbxLBpEsYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tạo list các id của tập validate"
      ],
      "metadata": {
        "id": "8plXImh19dft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_ids_validate = []\n",
        "with open(\"english_validate.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      output = source_tokenizer.encode(line)\n",
        "      id = output.ids\n",
        "      sos_id = source_tokenizer.token_to_id(\"<sos>\")\n",
        "      eos_id = source_tokenizer.token_to_id(\"<eos>\")\n",
        "      id = [sos_id] + id + [eos_id]\n",
        "      src_ids_validate.append(id)"
      ],
      "metadata": {
        "id": "t4iejyDSQM-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_ids_validate = []\n",
        "with open(\"vietnam_validate.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      output = target_tokenizer.encode(line)\n",
        "      id = output.ids\n",
        "      sos_id = target_tokenizer.token_to_id(\"<sos>\")\n",
        "      eos_id = target_tokenizer.token_to_id(\"<eos>\")\n",
        "      id = [sos_id] + id + [eos_id]\n",
        "      tgt_ids_validate.append(id)"
      ],
      "metadata": {
        "id": "uCdh3f6vQYoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding các vector id về cùng một kích thước"
      ],
      "metadata": {
        "id": "JJoGRmOk9sVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src = src_data\n",
        "        self.tgt = tgt_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'src': torch.tensor(self.src[idx], dtype=torch.long),\n",
        "            'tgt': torch.tensor(self.tgt[idx], dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "y8s5fVA-F_Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_pad_id = source_tokenizer.token_to_id(\"<pad>\")\n",
        "    tgt_pad_id = target_tokenizer.token_to_id(\"<pad>\")\n",
        "\n",
        "    src_batch = [item['src'] for item in batch]\n",
        "    tgt_batch = [item['tgt'] for item in batch]\n",
        "\n",
        "    src_pad = pad_sequence(src_batch, batch_first=True, padding_value=src_pad_id)\n",
        "    tgt_pad = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_pad_id)\n",
        "\n",
        "    return {\n",
        "        'src': src_pad,\n",
        "        'tgt': tgt_pad\n",
        "    }"
      ],
      "metadata": {
        "id": "uZEVvEumKDV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chỉnh sửa batch size"
      ],
      "metadata": {
        "id": "jQ8e0Qkt0S4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = TranslationDataset(src_ids, tgt_ids)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=0)"
      ],
      "metadata": {
        "id": "Y3Gse0gMK1Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate = TranslationDataset(src_ids_validate, tgt_ids_validate)\n",
        "valid_data = DataLoader(validate, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=0)"
      ],
      "metadata": {
        "id": "-btPLn6kQxDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kiểm tra thử dataloader và valid_data"
      ],
      "metadata": {
        "id": "J_X5VeL50Zov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader:\n",
        "    print(\"SRC batch:\")\n",
        "    print(batch['src'])\n",
        "    print(\"SRC shape:\", batch['src'].shape)\n",
        "\n",
        "    print(\"\\nTGT batch:\")\n",
        "    print(batch['tgt'])\n",
        "    print(\"TGT shape:\", batch['tgt'].shape)\n",
        "\n",
        "    break  # chỉ in 1 batch đầu tiên"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoihKYICLSC7",
        "outputId": "8f1533e9-a4c6-4bfb-e7a5-abb65d046b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC batch:\n",
            "tensor([[  2, 123, 435,  ...,   0,   0,   0],\n",
            "        [  2, 243, 160,  ...,   0,   0,   0],\n",
            "        [  2, 140, 125,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  2, 243, 284,  ...,   0,   0,   0],\n",
            "        [  2, 171, 155,  ...,   0,   0,   0],\n",
            "        [  2, 123, 541,  ...,   0,   0,   0]])\n",
            "SRC shape: torch.Size([64, 83])\n",
            "\n",
            "TGT batch:\n",
            "tensor([[  2, 874, 398,  ...,   0,   0,   0],\n",
            "        [  2, 481, 669,  ...,   0,   0,   0],\n",
            "        [  2, 162, 205,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  2, 445, 765,  ...,   0,   0,   0],\n",
            "        [  2, 365, 254,  ...,   0,   0,   0],\n",
            "        [  2, 169, 246,  ...,   0,   0,   0]])\n",
            "TGT shape: torch.Size([64, 90])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in valid_data:\n",
        "    print(\"SRC batch:\")\n",
        "    print(batch['src'])\n",
        "    print(\"SRC shape:\", batch['src'].shape)\n",
        "\n",
        "    print(\"\\nTGT batch:\")\n",
        "    print(batch['tgt'])\n",
        "    print(\"TGT shape:\", batch['tgt'].shape)\n",
        "\n",
        "    break  # chỉ in 1 batch đầu tiên"
      ],
      "metadata": {
        "id": "vyOKILagRQgZ",
        "outputId": "4d2422e5-703c-4c76-c875-999e591f436a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC batch:\n",
            "tensor([[   2,  165, 1062,  ...,    0,    0,    0],\n",
            "        [   2,  189,  155,  ...,   11,    3,    0],\n",
            "        [   2,  151, 3660,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   2,  221,  398,  ...,    0,    0,    0],\n",
            "        [   2,  123,    6,  ...,    0,    0,    0],\n",
            "        [   2,  140,  189,  ...,    0,    0,    0]])\n",
            "SRC shape: torch.Size([64, 74])\n",
            "\n",
            "TGT batch:\n",
            "tensor([[   2,  432,  246,  ...,    0,    0,    0],\n",
            "        [   2,  395,  245,  ...,    0,    0,    0],\n",
            "        [   2,  265, 2509,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   2,  301,  182,  ...,    0,    0,    0],\n",
            "        [   2,  169,  246,  ...,    0,    0,    0],\n",
            "        [   2,  162,  319,  ...,    0,    0,    0]])\n",
            "TGT shape: torch.Size([64, 114])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cài đặt mô hình"
      ],
      "metadata": {
        "id": "ktKiVeCvNuAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BaseSeq2Seq(nn.Module):\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"Train-time forward pass with teacher forcing\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate(self, src, max_len=100):\n",
        "        \"\"\"Inference-time decoding\"\"\"\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "OpeVSF3uNtHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_src = source_tokenizer.get_vocab()\n",
        "vocab_tgt = target_tokenizer.get_vocab()"
      ],
      "metadata": {
        "id": "oBrYWXTDZ7Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Source tokenizer vocab size:\", source_tokenizer.get_vocab_size())\n",
        "print(\"Target tokenizer vocab size:\", target_tokenizer.get_vocab_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh8fRbDUm5xu",
        "outputId": "d6150855-8e2a-4268-b7d5-ea5bccdfab5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source tokenizer vocab size: 16000\n",
            "Target tokenizer vocab size: 16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm huấn luyện và validate"
      ],
      "metadata": {
        "id": "Vwg3Tn8BULl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import wandb"
      ],
      "metadata": {
        "id": "zAyrtJJU3vsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import torch\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(\">> Start training\")\n",
        "    loop = tqdm(dataloader, total=len(dataloader), desc=\"training\", leave=True, colour=\"green\")\n",
        "    for batch in loop:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt = batch['tgt'].to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "\n",
        "        output = output.reshape(-1, output.shape[-1])\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_output)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        # Cập nhật thanh progress bar\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "      loop = tqdm(dataloader, total=len(dataloader), desc=\"validating\", leave=True, colour=\"blue\")\n",
        "      for batch in loop:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt = batch['tgt'].to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        output = model(src, tgt_input, teacher_forcing_ratio=0.0)  # No teacher forcing during validation\n",
        "        output = output.reshape(-1, output.shape[-1])\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "        loss = criterion(output, tgt_output)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        wandb.log({\"val_batch_loss\": loss.item()})\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# utils.py\n",
        "def get_model(name, config):\n",
        "    if name == 'lstm':\n",
        "        return\n",
        "    elif name == 'transformer':\n",
        "        return Seq2SeqTransformer(**config)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model name: {name}\")\n"
      ],
      "metadata": {
        "id": "ZoWP1Nw_T9yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cài đặt mô hình Transformers"
      ],
      "metadata": {
        "id": "GUvY6iR8f_4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "tnoA0kNrgBQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast"
      ],
      "metadata": {
        "id": "tFKepyP6nPZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(BaseSeq2Seq):\n",
        "    def __init__(self, input_dim, output_dim, embed_dim, num_heads, hidden_dim, num_layers, src_pad_idx, tgt_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.src_tok_emb = nn.Embedding(input_dim, embed_dim, padding_idx=src_pad_idx)\n",
        "        self.tgt_tok_emb = nn.Embedding(output_dim, embed_dim, padding_idx=tgt_pad_idx)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout= 0.3)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout= 0.3)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(self.device)\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=1.0):\n",
        "        src_mask = None\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        src_padding_mask = (src == self.src_pad_idx)\n",
        "        tgt_padding_mask = (tgt == self.tgt_pad_idx)\n",
        "\n",
        "        src = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "\n",
        "        memory = self.encoder(src.transpose(0, 1), src_key_padding_mask=src_padding_mask)\n",
        "        output = self.decoder(\n",
        "            tgt.transpose(0, 1),\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask\n",
        "        )\n",
        "\n",
        "        output = self.fc_out(output.transpose(0, 1))  # [B, T, V]\n",
        "        return output\n",
        "\n",
        "    # greedy encoding\n",
        "    def generate(self, src, max_len=50, beam_size=1):\n",
        "      self.eval()\n",
        "      with torch.no_grad(), autocast():\n",
        "          batch_size = src.size(0)\n",
        "          device = src.device\n",
        "          sos_id = target_tokenizer.token_to_id(\"<sos>\")\n",
        "          eos_id = target_tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "          self.to(device)\n",
        "          src_mask = None\n",
        "          src_padding_mask = (src == self.src_pad_idx).to(device)\n",
        "          src_embed = self.positional_encoding(self.src_tok_emb(src))\n",
        "          memory = self.encoder(src_embed.transpose(0, 1), src_key_padding_mask=src_padding_mask)\n",
        "          memory = memory.transpose(0, 1)\n",
        "\n",
        "          sequences = torch.full((batch_size, 1), sos_id, dtype=torch.long, device=device)\n",
        "          ended = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "          for _ in range(max_len):\n",
        "              tgt_mask = self.make_tgt_mask(sequences).to(device)\n",
        "              tgt_embed = self.positional_encoding(self.tgt_tok_emb(sequences))\n",
        "              out = self.decoder(\n",
        "                  tgt_embed.transpose(0, 1),\n",
        "                  memory.transpose(0, 1),\n",
        "                  tgt_mask=tgt_mask,\n",
        "                  memory_key_padding_mask=src_padding_mask\n",
        "              )\n",
        "              out = self.fc_out(out.transpose(0, 1))[:, -1, :]\n",
        "              probs = torch.softmax(out, dim=-1)\n",
        "              next_tokens = probs.argmax(dim=-1)\n",
        "              sequences = torch.cat([sequences, next_tokens.unsqueeze(1)], dim=1)\n",
        "              ended = ended | (next_tokens == eos_id)\n",
        "              if ended.all():\n",
        "                  break\n",
        "\n",
        "          return sequences\n"
      ],
      "metadata": {
        "id": "N3wn6QyXghbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "wandb.init(\n",
        "    project= \"nmt-transformer\",\n",
        "    name= \"nmt-transformer-run-4\"\n",
        ")\n",
        "config = {\n",
        "    \"input_dim\": len(vocab_src),         # số token trong vocab nguồn\n",
        "    \"output_dim\": len(vocab_tgt),        # số token trong vocab đích\n",
        "    \"embed_dim\": 256,\n",
        "    \"hidden_dim\": 512,\n",
        "    \"num_layers\": 6,\n",
        "    \"num_heads\": 8,\n",
        "    \"src_pad_idx\": vocab_src[\"<pad>\"],\n",
        "    \"tgt_pad_idx\": vocab_tgt[\"<pad>\"],\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "}\n",
        "num_epochs = 16\n"
      ],
      "metadata": {
        "id": "tIU64La-it9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "50737e20-5bdd-4eb8-91cb-a9e2ba80658d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m23020350duong\u001b[0m (\u001b[33m23020350duong-vietnam-national-university-hanoi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250627_085651-gmi1zhdu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer/runs/gmi1zhdu' target=\"_blank\">nmt-transformer-run-4</a></strong> to <a href='https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer' target=\"_blank\">https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer/runs/gmi1zhdu' target=\"_blank\">https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer/runs/gmi1zhdu</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(\"transformer\", config).to(config[\"device\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=config[\"tgt_pad_idx\"])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
        "patience = 3\n",
        "counter = 0\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch:',epoch+1)\n",
        "    train_loss = train_epoch(model, dataloader, optimizer, criterion, config[\"device\"])\n",
        "    wandb.log({\"Epoch\": epoch+1, \"train_loss\": train_loss})\n",
        "\n",
        "    val_loss = validate_epoch(model, valid_data, criterion, \"cuda\")\n",
        "    wandb.log({\"epoch\": epoch+1, \"val_loss\": val_loss, \"learning_rate\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      counter = 0\n",
        "      torch.save(model.state_dict(), 'best_model_2.pt')\n",
        "    else:\n",
        "      counter += 1\n",
        "      if counter >= patience:\n",
        "        print('early stopping')\n",
        "        break\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I641SK-mi0VC",
        "outputId": "5cebb3fe-58ed-4284-e611-eede12c5dc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            ">> Start training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   0%|\u001b[32m          \u001b[0m| 0/1563 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:17<00:00,  4.93it/s, loss=4.44]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 20.07it/s, loss=4.76]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2\n",
            ">> Start training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:18<00:00,  4.90it/s, loss=4.25]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.89it/s, loss=4.63]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3\n",
            ">> Start training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:17<00:00,  4.92it/s, loss=3.98]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.92it/s, loss=4.53]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4\n",
            ">> Start training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:18<00:00,  4.92it/s, loss=3.7]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.66it/s, loss=4.01]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5\n",
            ">> Start training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:17<00:00,  4.92it/s, loss=3.49]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.83it/s, loss=3.91]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6\n",
            ">> Start training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:16<00:00,  4.94it/s, loss=3.51]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.74it/s, loss=3.94]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7\n",
            ">> Start training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:17<00:00,  4.93it/s, loss=3.45]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 20.22it/s, loss=4]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:19<00:00,  4.89it/s, loss=3.04]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.81it/s, loss=3.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:17<00:00,  4.92it/s, loss=3.24]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 20.25it/s, loss=4.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:19<00:00,  4.89it/s, loss=3.25]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.91it/s, loss=3.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:18<00:00,  4.91it/s, loss=3.05]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.84it/s, loss=3.66]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:17<00:00,  4.92it/s, loss=2.89]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:08<00:00, 19.58it/s, loss=3.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:17<00:00,  4.92it/s, loss=2.92]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.73it/s, loss=2.88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:16<00:00,  4.93it/s, loss=2.96]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.71it/s, loss=3.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:19<00:00,  4.90it/s, loss=2.84]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.94it/s, loss=3.29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16\n",
            ">> Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|\u001b[32m██████████\u001b[0m| 1563/1563 [05:18<00:00,  4.91it/s, loss=2.62]\n",
            "validating: 100%|\u001b[34m██████████\u001b[0m| 157/157 [00:07<00:00, 19.78it/s, loss=2.85]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>batch_loss</td><td>██▇▆▅▆▅▅▅▅▄▄▄▄▄▃▃▃▂▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_batch_loss</td><td>▇██▆▇▄▄▅▆▅▄▄▃▃▂▄▅▄▂▃▂▂▃▂▂▂▂▃▁▃▂▁▂▃▁▂▂▂▂▄</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>16</td></tr><tr><td>batch_loss</td><td>2.62288</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_loss</td><td>2.76505</td></tr><tr><td>val_batch_loss</td><td>2.84606</td></tr><tr><td>val_loss</td><td>3.30806</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">nmt-transformer-run-4</strong> at: <a href='https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer/runs/gmi1zhdu' target=\"_blank\">https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer/runs/gmi1zhdu</a><br> View project at: <a href='https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer' target=\"_blank\">https://wandb.ai/23020350duong-vietnam-national-university-hanoi/nmt-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250627_085651-gmi1zhdu/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test nhỏ lẻ"
      ],
      "metadata": {
        "id": "L2CjkxFs8-6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_model_2.pt\", map_location='cuda'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "2NYe6o58rh6I",
        "outputId": "a91b3156-1d3b-4d39-d397-b32fe8ec87df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqTransformer(\n",
              "  (src_tok_emb): Embedding(16000, 256, padding_idx=0)\n",
              "  (tgt_tok_emb): Embedding(16000, 256, padding_idx=0)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        (dropout3): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc_out): Linear(in_features=256, out_features=16000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"we are playing on the ground\"\n",
        "encode_sentence = source_tokenizer.encode(sentence)\n",
        "id = encode_sentence.ids\n",
        "src_id = [source_tokenizer.token_to_id('<sos>')] + id + [source_tokenizer.token_to_id('<eos>')]\n",
        "# Đưa vào tensor\n",
        "src_tensor = torch.tensor([src_id], dtype=torch.long).to('cuda')"
      ],
      "metadata": {
        "id": "rRiZyAhH8ahJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = model.generate(src_tensor, max_len=100)\n",
        "output = target_tokenizer.decode(output_ids.squeeze().tolist())\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "uQ6iedDV9t6S",
        "outputId": "64eb0f1b-7f01-4c17-a6c5-6b13c0c7f2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-905772382.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' chúng ta đang chơi trên mặt đất.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacrebleu"
      ],
      "metadata": {
        "id": "dK2XU3l0Yuj1",
        "outputId": "6754a786-8150-434b-e1ac-06ffd80257da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "def compute_bleu(model, dataloader, device, tokenizer, config, num_examples_to_print=5):\n",
        "    model.eval()\n",
        "    hypotheses = []\n",
        "    references = []\n",
        "\n",
        "    config[\"tgt_pad_idx\"] = target_tokenizer.token_to_id(\"<pad>\")\n",
        "    config[\"tgt_sos_idx\"] = target_tokenizer.token_to_id(\"<sos>\")\n",
        "    config[\"tgt_eos_idx\"] = target_tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "    special_tokens = {\n",
        "        config[\"tgt_pad_idx\"],\n",
        "        config[\"tgt_sos_idx\"],\n",
        "        config[\"tgt_eos_idx\"]\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            output_ids = model.generate(src, max_len=100)\n",
        "\n",
        "            for ref_ids, hyp_ids in zip(tgt.tolist(), output_ids.tolist()):\n",
        "                # Loại bỏ token đặc biệt\n",
        "                ref_tokens = [t for t in ref_ids if t not in special_tokens]\n",
        "                hyp_tokens = [t for t in hyp_ids if t not in special_tokens]\n",
        "\n",
        "                ref_text = tokenizer.decode(ref_tokens)\n",
        "                hyp_text = tokenizer.decode(hyp_tokens)\n",
        "\n",
        "                references.append(ref_text)\n",
        "                hypotheses.append(hyp_text)\n",
        "\n",
        "    # BLEU yêu cầu references là list của list\n",
        "    bleu = corpus_bleu(hypotheses, [references])\n",
        "\n",
        "    print(f\"\\n===> BLEU score: {bleu.score:.2f}\")\n",
        "\n",
        "    # Hiển thị một số cặp câu để kiểm tra\n",
        "    print(\"\\n===> Sample translations:\")\n",
        "    for i in range(min(num_examples_to_print, len(hypotheses))):\n",
        "        print(f\"REF: {references[i]}\")\n",
        "        print(f\"HYP: {hypotheses[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return bleu.score"
      ],
      "metadata": {
        "id": "krEltg745dSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = compute_bleu(model, valid_data, \"cuda\", target_tokenizer, config= config, num_examples_to_print=5)\n",
        "score"
      ],
      "metadata": {
        "id": "mnUA8Nc55fry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc66b54-3511-4461-ab50-511fe2c75a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-905772382.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===> BLEU score: 5.04\n",
            "\n",
            "===> Sample translations:\n",
            "REF:  ông không bao giờ yêu con, johnny.\n",
            "HYP:  ông không bao giờ yêu cầu bạn,..................... anh...... anh. anh. anh không.. anh. anh không bao giờ. anh.\n",
            "--------------------------------------------------\n",
            "REF:  tôi xin lỗi vince, tôi khá bận.\n",
            "HYP:  tôi xin lỗi về điều đó, v............................. tôi...........\n",
            "--------------------------------------------------\n",
            "REF:  tìm được mấy vỏ đạn này\n",
            "HYP:  và chúng tôi tìm thấy những người này............................... và chúng tôi tìm thấy và chúng tôi tìm thấy....\n",
            "--------------------------------------------------\n",
            "REF:  và những chủ quyền, có tính chất xuyên xuốt, liên kết với nhau một cách toàn cầu này, theo một vài cách nào đó thách thức chủ quyền của các quốc gia theo những cách rất thú vị nhưng đôi khi cũng hành động vượt ra và mở rộng nó vào khi mà điều khiển vượt quá những gì mọi người có thể hoặc không thể làm với thông tin có nhiều tác động hơn bao giờ hết đối với việc thi hành quyền lực trong thế giới vật chất.\n",
            "HYP:  và hệ thống của họ, các nhà hoạt động, các nhà hoạt động, các phương tiện giao tiếp, có thể thách thức một vài cách dễ dàng hơn trong lĩnh vực thẳm, nhưng đôi khi nó có thể làm việc với dự án, và khi có thể làm việc với một dự án tối ưu tiên tri thức và không thể làm gì trong việc với những thứ mà mọi người có thể làm trong thế giới hạn chế độ.\n",
            "--------------------------------------------------\n",
            "REF:  uther phải thấy lý do đó chứ.\n",
            "HYP:  các nhà toán học phải thấy lý do..................... người.. người...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.041041163822801"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}